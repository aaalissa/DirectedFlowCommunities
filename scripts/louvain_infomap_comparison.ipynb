{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: to be able to use all crisp methods, you need to install some additional packages:  {'graph_tool'}\n",
      "Note: to be able to use all crisp methods, you need to install some additional packages:  {'pyclustering'}\n"
     ]
    }
   ],
   "source": [
    "import sys; sys.path.append(\"..\")\n",
    "\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "\n",
    "from utils.network_loader import read_communities\n",
    "from utils.dyetracing_classes import evaluate_clustering\n",
    "\n",
    "from utils.dyetracing_classes import *\n",
    "from utils.node_selection import find_low_centrality_nodes, iter_node_selection, find_low_betweenness_nodes\n",
    "from utils.network_loader import read_communities, load_nx_graph, setup_dyetracing_graph\n",
    "# from utils.boundary import detect_boundaries, is_distinct_boundary\n",
    "\n",
    "from cdlib import algorithms\n",
    "import csv\n",
    "import os\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing mu=10: 100%|██████████| 100/100 [00:33<00:00,  3.01it/s]\n",
      "Processing mu=20: 100%|██████████| 100/100 [00:33<00:00,  2.98it/s]\n",
      "Processing mu=30: 100%|██████████| 100/100 [00:33<00:00,  2.97it/s]\n",
      "Processing mu=40: 100%|██████████| 100/100 [00:33<00:00,  2.98it/s]\n",
      "Processing mu=50: 100%|██████████| 100/100 [00:33<00:00,  2.97it/s]\n",
      "Processing mu=60: 100%|██████████| 100/100 [00:33<00:00,  2.99it/s]\n",
      "Processing mu=70: 100%|██████████| 100/100 [00:32<00:00,  3.03it/s]\n",
      "Processing mu=80: 100%|██████████| 100/100 [00:34<00:00,  2.93it/s]\n",
      "Processing mu=90: 100%|██████████| 100/100 [00:35<00:00,  2.86it/s]\n"
     ]
    }
   ],
   "source": [
    "# Define the mu values to loop over\n",
    "mu_values = [10, 20, 30, 40, 50, 60, 70, 80, 90]\n",
    "\n",
    "for mu in mu_values:\n",
    "    # CSV file path\n",
    "    csv_file = f'{mu}mu_comparison.csv'\n",
    "\n",
    "    # Headers for the CSV file\n",
    "    headers = ['run', 'true # of comms', 'louvain # of pred comms', 'louvain accuracy', 'louvain ari', 'louvain nmi', \n",
    "               'infomap # of pred comms', 'infomap accuracy', 'infomap ari', 'infomap nmi', \n",
    "               'rbpots # of pred comms', 'rbpots accuracy', 'rbpots ari', 'rbpots nmi']\n",
    "\n",
    "    # Open the CSV file for writing\n",
    "    with open(csv_file, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(headers)\n",
    "\n",
    "        # Loop over i from 1 to 100 with a progress bar\n",
    "        for i in tqdm(range(1, 101), desc=f'Processing mu={mu}'):\n",
    "            louvain_community = f'/Users/alissachavalithumrong/Documents/research/flowcommunities/benchmarks/directed_louvain/500_node/{mu}/{i}_mu{mu}.tree'\n",
    "            community_file = f'/Users/alissachavalithumrong/Documents/research/flowcommunities/benchmarks/LF_created_networks/500_node/{mu}/{i}_community_N500_k50_maxk75_mu{mu}.dat'\n",
    "            network_file = f'/Users/alissachavalithumrong/Documents/research/flowcommunities/benchmarks/LF_created_networks/500_node/{mu}/{i}_network_N500_k50_maxk75_mu{mu}.dat'\n",
    "\n",
    "            group_membership = np.array(list(read_communities(community_file).values()))\n",
    "            louvain_membership = dict(sorted(read_communities(louvain_community).items(), key=lambda x: x[0]))\n",
    "            louvain_membership = np.array(list(louvain_membership.values()))\n",
    "\n",
    "            accuracy, ari, nmi = evaluate_clustering(group_membership, louvain_membership)\n",
    "            num_communities = len(np.unique(group_membership))\n",
    "            num_louvain_communities = len(np.unique(louvain_membership))\n",
    "\n",
    "            node_list, edge_list = setup_dyetracing_graph(network_file)\n",
    "            G_directed = load_nx_graph(node_list, network_file)\n",
    "\n",
    "            # Run baseline algorithms and get their evaluations\n",
    "            infomap_comms = algorithms.infomap(G_directed)\n",
    "            infomap_labels = np.array([next(i for i, comm in enumerate(infomap_comms.communities) if node in comm) for node in G_directed.nodes()])\n",
    "            infomap_accuracy, infomap_ari, infomap_nmi = evaluate_clustering(group_membership, infomap_labels)\n",
    "            num_infomap_comms = len(infomap_comms.communities)\n",
    "\n",
    "            rb_pots_comms = algorithms.rb_pots(G_directed)\n",
    "            rb_pots_labels = np.array([next(i for i, comm in enumerate(rb_pots_comms.communities) if node in comm) for node in G_directed.nodes()])\n",
    "            rb_pots_accuracy, rb_pots_ari, rb_pots_nmi = evaluate_clustering(group_membership, rb_pots_labels)\n",
    "            num_rb_pots_comms = len(rb_pots_comms.communities)\n",
    "\n",
    "            # Write the results to the CSV file\n",
    "            writer.writerow([i, num_communities, num_louvain_communities, accuracy, ari, nmi, \n",
    "                             num_infomap_comms, infomap_accuracy, infomap_ari, infomap_nmi, \n",
    "                             num_rb_pots_comms, rb_pots_accuracy, rb_pots_ari, rb_pots_nmi])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
